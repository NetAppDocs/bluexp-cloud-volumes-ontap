---
sidebar: sidebar
permalink: add-new-nic.html
keywords: NIC, networking, SnapMirror replication, add, additional NIC, new NIC
summary: You can separate networking traffic in SnapMirror replication relationships with a new NIC and Intercluster LIF. 
---

= Add a new NIC in Azure
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
You can separate networking traffic in SnapMirror replication relationships with a new NIC and Intercluster LIF. 

== Create an additional NIC and attach to the destination VM

.Steps
. In ONTAP CLI, stop the node.
+
[source,json]
----
dest::> halt -node <dest_node-vm>
----
. In Azure Bash Shell, stop the node.
.. `az vm stop --resource-group <dest_node-rg> --name <dest_node-vm>``
.. `az vm deallocate --resource-group <dest_node-rg> --name <dest_node-vm>``

. Add a new NIC (e.g., nic-new)
.. Create the new NIC on the VM. *(Rachel: Is there only one VM or should it be defined?)*
+
The _security group_ should be defined such that the new subnet is non-routable to the existing subnet attached to NIC0.
+ 
`az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --vnet-name <vnet> --subnet <src_subnet> --network-security-group <security group> --accelerated-networking true``
.. If the VNET is in a different resource group than the vm
... `az network vnet subnet show -g <src_vnet-rg> -n <src_subnet> --vnet-name <vnet> --query id``
... `az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --subnet <id_from_prev_command> --accelerated-networking true``

. Attach New NIC to the VM
+
`az vm nic add -g <dest_node-rg> --vm-name <dest_node-vm> --nics <dest_node-vm-nic-new>``

. Start the node
+
`az vm start --resource-group <dest_node-rg>  --name <dest_node-vm>``

. Confirm that the second NIC, e.g. nic-new exists and accelerated networking is enabled

Repeat the steps for the partner node in case of HA.

== Create a separate IPSpace and broadcast domain for the new NIC
IPSpaces can help provide logical separation between networking functionality. We created a separate IPSpace for Intercluster LIFs for replication between clusters.

Use the ONTAP CLI to do the following steps.

.Steps

. Create a IPSpace and Broadcast domain for the new nic-new, and also create a intercluster LIF with nic-new on destination:

.. Create IPSpace
+
`dest::> network ipspace create -ipspace <new_ipspace>``

.. Create Broadcast domain on new_ipspace Ipspace and add nic-new ports
+
`dest::> Network port show``

.. Use any of the unused ports. For single node systems, the port is _e0b_. For HA-pair deployments with managed disks, the port is _e0d_. For HA-pair deployments with page blobs, the port is _e0e_. Also note that node name can be different from the VM name.
+
`dest::> broadcast-domain create -broadcast-domain <new_bd> -mtu 1500 -ipspace <new_ipspace> -ports <dest_node-cot-vm:e0b>``

.. Create intercluster LIF on <new_bd> broadcast-domain and the new NIC, e.g. nic-new
+
`dest::> net int create -vserver <new_ipspace> -lif <new_dest_node-ic-lif> -service-policy default-intercluster -address <new_added_nic_primary_addr> -home-port <e0b> -home-node <node> -netmask <new_netmask_ip> -broadcast-domain <new_bd> ``

For HA-pair deployments, repeat steps 2 and 3 for the partner node.

== Create cluster peering between the source and destination system
.Steps

. Make sure the intercluster LIF of the destination System is able to talk to the intercluster LIF of the source cluster.
+
`dest::> ping -lif <new_dest_node-ic-lif> -vserver <new_ipspace> -destination <10.161.189.6>`` (destination is inter cluster LIF Ip addr on Source)

. Make sure the intercluster LIF of the source is able to talk to the intercluster LIF of the destination cluster
+
`src::> ping -lif <src_node-ic-lif> -vserver <src_svm> -destination <10.161.189.18>`` (destination is the IP address of the new NIC created on Destination)

For HA-pair deployments, repeat the steps for the partner node.

== Create vserver peering between the source and destination system
.Steps

. Create Cluster Peering on destination
+
`dest::> cluster peer create -peer-addrs <10.161.189.6> -ipspace <new_ipspace>``

. Create Cluster Peering on source
+
`src::> cluster peer create -peer-addrs <10.161.189.18>, (<partner_new_nic_ip_addr> for HA)``

. Check Cluster peer successful
+
`src::> cluster peer show`` (Availability field should show available)

. Create Vserver Peering on destination, both source and destination vserver should be data vserver
.. See the list of vservers and the cluster name on source and dest (vserver show, cluster identity show)  
.. Run `dest::> vserver peer create -vserver <dest_svm> -peer-vserver <src_svm> -peer-cluster <src_cluster> -applications snapmirror``

. Accept Vserver Peering
+
`src::> vserver peer accept -vserver <src_svm> -peer-vserver <dest_svm>``

. Check Vserver Peering successful
+
`Vserver Peer show`` (Peer state should show peered and peering application should show Snapmirror)

== Create a SnapMirror relationship between the source and destination system

.Steps
. Create data protected volume on the destination vserver
+
.. `dest::> vol create -volume <new_dest_vol> -vserver <dest_svm> -type DP -size <10GB> -aggregate <aggr1>``
.. `dest::> vserver export-policy rule create -clientmatch 0.0.0.0/0 -policyname default -vserver <dest_svm> -rwrule any -allow-dev true -superuser any -allow-suid true -rorule any``

. Create and initialize snapmirror relationship on destination, choose the policy and schedule according to the requirements
.. `dest::> snapmirror create -source-path <src_svm:src_vol>  -destination-path  <dest_vs:new_dest_vol> -vserver <dest_svm> -policy <MirrorAllSnapshots> -schedule <5min>``
.. `dest::> snapmirror initialize -destination-path  <dest_vs:new_dest_vol>``

== Validate the SnapMirror relationship is healthy
In the ONTAP CLI, run the following commands to valide if the SnapMirror relationship is healthy. 

[cols=2*,options="header",cols="20,30"]
|===

| Command
| Output

| snapmirror show | healthy
| snapmirror show-history | successful creation and initialization

|===
If you check after the scheduled time has passed it should show a successful update as well

Optionaly you can mount the source and destination vols using "vol mount", write a file to the source and verify it being replicated to destination.