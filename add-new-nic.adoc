---
sidebar: sidebar
permalink: add-new-nic.html
keywords: NIC, networking, SnapMirror replication, add, additional NIC, new NIC
summary: You can separate networking traffic in SnapMirror replication relationships with a new NIC and Intercluster LIF. 
---

= Add a new NIC in Azure
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
#2: SnapMirror replication relationships typically replicate data between two routable subnets. You can segregate the replication to nonroutable subnets by adding a new network interface card (NIC). 

#1: You can separate network traffic in SnapMirror replication relationships by adding a network interface card (NIC) and Intercluster LIF in a nonroutable subnet. The separation allows you to back your data up using SnapMirror replication to a destination Cloud Volumes ONTAP which is on a different subnet using a new NIC. 

NOTE: The storage VM instance type you deploy must have an additional and unused NIC to add a new NIC.

== Create an additional NIC and attach to the destination VM

.Steps
. In the ONTAP CLI, stop the node.
+
[source,json]
----
dest::> halt -node <dest_node-vm>
----
. In the Azure portal, check that the VM (node) status is stopped. 
. Use the Bash environment in Azure Cloud Shell to stop the node.
.. Stop the resource group.
+
[source,json]
----
az vm stop --resource-group <dest_node-rg> --name <dest_node-vm>
----
.. De-allocate the resource group.
+ 
[source,json]
----
az vm deallocate --resource-group <dest_node-rg> --name <dest_node-vm>
----
. Add a new NIC (e.g., nic-new)
.. Create the new NIC on the VM. 
+
The _security group_ should be defined such that the new subnet is non-routable to the existing subnet attached to NIC0.
 
... Look up the subnet ID for the resource group. 
+
[source,json]
----
az network vnet subnet show -g <src_vnet-rg> -n <src_subnet> --vnet-name <vnet> --query id
----
... Create the new NIC on the VM with the subnet ID. Here you enter the name for the new NIC. 
+
[source,json]
----
az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --subnet <id_from_prev_command> --accelerated-networking true
----
+
NOTE: Save the privateIPAddress. This IP address, <new_netmask_ip>, is used in Create a separate IPSpace and broadcast domain for the new NIC.

. Attach the new NIC to the VM.
+
[source,json]
----
az vm nic add -g <dest_node-rg> --vm-name <dest_node-vm> --nics <dest_node-vm-nic-new>
----
. Start the VM (node).
+
[source,json]
----
az vm start --resource-group <dest_node-rg>  --name <dest_node-vm>
----
. In the Azure portal, go to *Networking* and confirm that the new NIC, e.g. nic-new, exists and accelerated networking is enabled. 

Repeat the steps for the partner node in case of HA.

== Create a separate IPSpace and broadcast domain for the new NIC
A separate IPSpace for intercluster LIFs provides logical separation between networking functionality for replication between clusters. 

Use the ONTAP CLI to do the following steps.

.Steps
. Create an IPSpace and broadcast domain for the new NIC (nic-new), and also create an intercluster LIF with the new NIC (nic-new) on destination:

.. Create the new IPSpace (new_ipspace).
+
[source,json]
----
dest::> network ipspace create -ipspace <new_ipspace>
----
.. Create a broadcast domain on the new IPSpace (new_ipspace) and add nic-new ports.
+
[source,json]
----
dest::> network port show
----
.. Use any of the unused ports. For single node systems, the port is _e0b_. For HA-pair deployments with managed disks, the port is _e0d_. For HA-pair deployments with page blobs, the port is _e0e_. Use the node name not the VM name. Find the node name by running `node show`.  
+
[source,json]
----
dest::> broadcast-domain create -broadcast-domain <new_bd> -mtu 1500 -ipspace <new_ipspace> -ports <dest_node-cot-vm:e0b>
----
.. Create an intercluster LIF on the new broadcast-domain (new_bd) and on the new NIC (nic-new).
+
[source,json]
----
dest::> net int create -vserver <new_ipspace> -lif <new_dest_node-ic-lif> -service-policy default-intercluster -address <new_added_nic_primary_addr> -home-port <e0b> -home-node <node> -netmask <new_netmask_ip> -broadcast-domain <new_bd>
----

.. Verify creation of the new intercluster LIF.
[source,json]
----
dest::>net int show
----

For HA-pair deployments, repeat steps 2 and 3 for the partner node.

== Create cluster peering between the source and destination systems
.Steps

. Make sure the destination system intercluster LIF can talk to the intercluster LIF of the source cluster. The destination is the intercluster LIF IP address on the source. 
+
[source,json]
----
dest::> ping -lif <new_dest_node-ic-lif> -vserver <new_ipspace> -destination <10.161.189.6> 
----
. Make sure the intercluster LIF of the source can talk to the intercluster LIF of the destination cluster. The destination is the IP address of the new NIC created on the destination. 
+
[source,json]
----
src::> ping -lif <src_node-ic-lif> -vserver <src_svm> -destination <10.161.189.18>
----

For HA-pair deployments, repeat the steps for the partner node.

== Create vserver peering between the source and destination system
.Steps

. Create cluster peering on the destination.
+
[source,json]
----
dest::> cluster peer create -peer-addrs <10.161.189.6> -ipspace <new_ipspace>
----
. Create cluster peering on the source. For systems with HA pairs, use <partner_new_nic_ip_addr> for the -peer-addrs. *(Rachel - Does this last sentence make sense?)*
+
[source,json]
----
src::> cluster peer create -peer-addrs <10.161.189.18>
----
. Check that the cluster peered.
+
[source,json]
----
src::> cluster peer show 
----
.Output
Successful peering shows available in the availability field. 
. Create Vserver peering on the destination. Both source and destination Vservers should be data Vservers.
.. View the list of Vservers and the cluster name on source and destination *Rachel - Why is this step here? Is it necessary? The actual code is for step 4 I think, not step 4.a.*  
+
[source,json]
----
dest::> vserver peer create -vserver <dest_svm> -peer-vserver <src_svm> -peer-cluster <src_cluster> -applications snapmirror``
----
. Accept the Vserver peering.
+
[source,json]
----
src::> vserver peer accept -vserver <src_svm> -peer-vserver <dest_svm>
----
. Check that the Vserver peered.
+
[source,json]
----
Vserver Peer show
----

.Result
Peer state should show `peered` and peering application should show `Snapmirror`. *Rachel - Is the output for `Vserver Peer show` correct as I have placed in mono font?*

== Create a SnapMirror relationship between the source and destination system
*Rachel - Why do this?*

.Steps
. Create a data protected volume on the destination vserver. 
+
[source,json]
----
dest::> vol create -volume <new_dest_vol> -vserver <dest_svm> -type DP -size <10GB> -aggregate <aggr1>
----
. Add an export policy rule to the volume. *(Rachel: Is this the correct step and is it necessary for all volumes?)* 
+
[source,json]
----
dest::> vserver export-policy rule create -clientmatch 0.0.0.0/0 -policyname default -vserver <dest_svm> -rwrule any -allow-dev true -superuser any -allow-suid true -rorule any``
----
. Create and initialize the SnapMirror replication relationship on the destination. 
+
[source,json]
----
dest::> snapmirror create -source-path <src_svm:src_vol>  -destination-path  <dest_vs:new_dest_vol> -vserver <dest_svm> -policy <MirrorAllSnapshots> -schedule <5min>
----
. Choose the SnapMirror policy and schedule according to the requirements.
+
[source,json]
----
dest::> snapmirror initialize -destination-path  <dest_vs:new_dest_vol>
----

== Validate the SnapMirror relationship is healthy
In the ONTAP CLI, run the following commands to validate the SnapMirror relationship is healthy. 

[cols=2*,options="header",cols="20,30"]
|===

| Command
| Output

| snapmirror show | Healthy
| snapmirror show-history | Successful creation and initialization

|===
If you check after the scheduled time has passed it should show a successful update as well (*Rachel: What is the scheduled time?)*

Optionally, you can mount the source and destination volumes using "vol mount", write a file to the source, and verify the volume is replicating to the destination. *(Rachel: Are there commands or specific instructions for how to do this)*