---
sidebar: sidebar
permalink: add-new-nic.html
keywords: NIC, networking, SnapMirror replication, add, additional NIC, new NIC
summary: You can separate networking traffic in SnapMirror replication relationships with a new NIC and Intercluster LIF. 
---

= Add a new NIC in Azure
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
You can separate networking traffic in SnapMirror replication relationships with a new network interface card (NIC) and Intercluster LIF. The separation allows you to back your data up using SnapMirror replication to a destination Cloud Volumes ONTAP which is on a different subnet using a new NIC. 

== Create an additional NIC and attach to the destination VM

.Steps
. In ONTAP CLI, stop the node.
+
[source,json]
----
dest::> halt -node <dest_node-vm>
----
. In the Azure portal, check that the VM status is offline/down. *(Rachel: How do you check this and is "offline/down" or other term correct?)*
. In Azure Bash Shell, stop the node.
.. Stop the resource group.
+
[source,json]
----
az vm stop --resource-group <dest_node-rg> --name <dest_node-vm>
----
.. De-allocate the resource group.
+ 
[source,json]
----
az vm deallocate --resource-group <dest_node-rg> --name <dest_node-vm>
----
. Add a new NIC (e.g., nic-new)
.. Create the new NIC on the VM. *(Rachel: Is there only one VM or should it be defined?)*
+
The _security group_ should be defined such that the new subnet is non-routable to the existing subnet attached to NIC0.
+
[source,json]
---- 
az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --vnet-name <vnet> --subnet <src_subnet> --network-security-group <security group> --accelerated-networking true
----
.. If the VNET is in a different resource group than the VM, run the following commands: 
... Look up the subnet ID for the resource group. 
+
[source,json]
----
az network vnet subnet show -g <src_vnet-rg> -n <src_subnet> --vnet-name <vnet> --query id
----
... Create the new NIC on the VM with the subnet ID.
+
[source,json]
----
az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --subnet <id_from_prev_command> --accelerated-networking true
----
. Attach New NIC to the VM
+
[source,json]
----
az vm nic add -g <dest_node-rg> --vm-name <dest_node-vm> --nics <dest_node-vm-nic-new>
----
. Start the node
+
[source,json]
----
az vm start --resource-group <dest_node-rg>  --name <dest_node-vm>
----
. Confirm that the second NIC, e.g. nic-new, exists and accelerated networking is enabled. *(Rachel: How do users confirm this?)*

Repeat the steps for the partner node in case of HA.

== Create a separate IPSpace and broadcast domain for the new NIC
A separate IPSpace for intercluster LIFs provides logical separation between networking functionality for replication between clusters. 

Use the ONTAP CLI to do the following steps.

.Steps

. Create an IPSpace and broadcast domain for the new NIC (nic-new), and also create an intercluster LIF with the new NIC (nic-new) on destination:

.. Create the new IPSpace (new_ipspace).
+
[source,json]
----
dest::> network ipspace create -ipspace <new_ipspace>
----
.. Create a broadcast domain on the new IPSpace (new_ipspace) and add nic-new ports.
+
[source,json]
----
dest::> Network port show
----
.. Use any of the unused ports. For single node systems, the port is _e0b_. For HA-pair deployments with managed disks, the port is _e0d_. For HA-pair deployments with page blobs, the port is _e0e_. Also note that node name can be different from the VM name.
+
[source,json]
----
dest::> broadcast-domain create -broadcast-domain <new_bd> -mtu 1500 -ipspace <new_ipspace> -ports <dest_node-cot-vm:e0b>
----
.. Create an intercluster LIF on the new broadcast-domain (new_bd) and on the new NIC (nic-new).
+
[source,json]
----
dest::> net int create -vserver <new_ipspace> -lif <new_dest_node-ic-lif> -service-policy default-intercluster -address <new_added_nic_primary_addr> -home-port <e0b> -home-node <node> -netmask <new_netmask_ip> -broadcast-domain <new_bd>
----

For HA-pair deployments, repeat steps 2 and 3 for the partner node.

== Create cluster peering between the source and destination system
.Steps

. Make sure the destination system intercluster LIF can talk to/connect with the intercluster LIF of the source cluster. The destination is the intercluster LIF IP addr on the source. *(Rachel - Does this last sentence make sense?)*
+
[source,json]
----
dest::> ping -lif <new_dest_node-ic-lif> -vserver <new_ipspace> -destination <10.161.189.6> 
----
. Make sure the intercluster LIF of the source can talk to the intercluster LIF of the destination cluster. The destination is the IP address of the new NIC created on the destination. *(Rachel - Does this last sentence make sense?)*
+
[source,json]
----
src::> ping -lif <src_node-ic-lif> -vserver <src_svm> -destination <10.161.189.18>
----

For HA-pair deployments, repeat the steps for the partner node.

== Create vserver peering between the source and destination system
.Steps

. Create cluster peering on the destination.
+
[source,json]
----
dest::> cluster peer create -peer-addrs <10.161.189.6> -ipspace <new_ipspace>
----
. Create cluster peering on the source. For systems with HA pairs, use <partner_new_nic_ip_addr> for the -peer-addrs. *(Rachel - Does this last sentence make sense?)*
+
[source,json]
----
src::> cluster peer create -peer-addrs <10.161.189.18>
----
. Check that the cluster peered.
+
[source,json]
----
src::> cluster peer show 
----
.Output
Successful peering shows available in the availability field. 
. Create Vserver peering on the destination. Both source and destination vservers should be data vservers.
.. View the list of vservers and the cluster name on source and destination *(vserver show, cluster identity show) - Why is this here?*  
+
[source,json]
----
dest::> vserver peer create -vserver <dest_svm> -peer-vserver <src_svm> -peer-cluster <src_cluster> -applications snapmirror``
----
. Accept the Vserver peering.
+
[source,json]
----
src::> vserver peer accept -vserver <src_svm> -peer-vserver <dest_svm>
----
. Check that the Vserver peered.
+
[source,json]
----
`Vserver Peer show`` (Peer state should show peered and peering application should show Snapmirror)
----

== Create a SnapMirror relationship between the source and destination system
*Rachel - Why do this?*

.Steps
. Create a data protected volume on the destination vserver. 
+
[source,json]
----
dest::> vol create -volume <new_dest_vol> -vserver <dest_svm> -type DP -size <10GB> -aggregate <aggr1>
----
. Add an export policy rule to the volume. *(Rachel: Is this the correct step and is it necessary for all volumes?)* 
+
[source,json]
----
dest::> vserver export-policy rule create -clientmatch 0.0.0.0/0 -policyname default -vserver <dest_svm> -rwrule any -allow-dev true -superuser any -allow-suid true -rorule any``
----
. Create and initialize the SnapMirror replication relationship on the destination. 
+
[source,json]
----
dest::> snapmirror create -source-path <src_svm:src_vol>  -destination-path  <dest_vs:new_dest_vol> -vserver <dest_svm> -policy <MirrorAllSnapshots> -schedule <5min>
----
. Choose the SnapMirror policy and schedule according to the requirements.
+
[source,json]
----
dest::> snapmirror initialize -destination-path  <dest_vs:new_dest_vol>
----

== Validate the SnapMirror relationship is healthy
In the ONTAP CLI, run the following commands to validate the SnapMirror relationship is healthy. 

[cols=2*,options="header",cols="20,30"]
|===

| Command
| Output

| snapmirror show | healthy
| snapmirror show-history | successful creation and initialization

|===
If you check after the scheduled time has passed it should show a successful update as well (*Rachel: What is the scheduled time?)*

Optionally, you can mount the source and destination volumes using "vol mount", write a file to the source, and verify the volume is replicating to the destination. *(Rachel: Are there commands or specific instructions for how to do this)*