---
sidebar: sidebar
permalink: segregate-snapmirror-azure.html
keywords: segregate, SnapMirror, SnapMirror traffic, SnapMirror replication, add, additional NIC, new NIC, intercluster LIF, non-routable subnet, subnet
summary: You can segregate SnapMirror replication traffic with a new NIC and Intercluster LIF in a non-routable subnet. 
---

= Segregate SnapMirror traffic in Azure
:hardbreaks:
:nofooter:
:icons: font
:linkattrs:
:imagesdir: ./media/

[.lead]
In Cloud Volumes ONTAP deployments with SnapMirror replication, data typically replicates between two routable subnets. With Cloud Volumes ONTAP in Azure, you can segregate SnapMirror replication traffic from your data traffic by adding a new network interface card (NIC), an associated intercluster LIF and a non-routable subnet. 

== Visualize the segregation of SnapMirror replication traffic in single node and high-availability pair configurations

.Figure 1
The following diagram illustrates the segregation of SnapMirror replication traffic in a single node configuration. 

image:diagram-snapmirror-segregation-azure-SN.png[Diagram illustrates the segregation of SnapMirror replication traffic in a single node configuration]

.Figure 2
The following diagram illustrates the segregation of SnapMirror replication traffic in a high-availability pair configuration. 

image:diagram-snapmirror-segregation-azure-HA.png[Diagram illustrates the segregation of SnapMirror replication traffic in a high-availability pair configuration]

== Considerations
This section provides considerations for segregating SnapMirror replication traffic. 

* The new SnapMirror subnet must be non-routable to the existing data subnet. This can be achieved by configuring network security group rules. 
* You can only add a single NIC to a VM for SnapMirror traffic segregation.
* To add a new NIC, the storage VM (SVM) instance type you deploy must have an unused NIC.

== Before you begin
The source and destination clusters must be in the same VNET (Virtual Network) but not in the same subnet. 

== Create an additional NIC and attach to the destination VM
This section provides instructions for how to create an additional NIC and attach it to the destination VM. 

.Steps
. In the ONTAP CLI, stop the node.
+
[source]
----
dest::> halt -node <dest_node-vm>
----
. In the Azure portal, check that the VM (node) status is stopped. 
+
[source]
----
az vm get-instance-view --resource-group <dest-rg> --name <dest-vm> --query instanceView.statuses[1].displayStatus
----

. Use the Bash environment in Azure Cloud Shell to stop the node.
.. Stop the node.
+
[source]
----
az vm stop --resource-group <dest_node-rg> --name <dest_node-vm>
----
.. De-allocate the node.
+ 
[source]
----
az vm deallocate --resource-group <dest_node-rg> --name <dest_node-vm>
----

. Configure network security group rules to make the two subnets (source cluster subnet and destination cluster subnet) non-routable to each other.  
.. Create the new NIC on the destination VM. 
+
The _security group_ should be defined such that the destination cluster subnet is non-routable to the source cluster subnet.
 
... Look up the subnet ID for the source cluster subnet. 
+
[source]
----
az network vnet subnet show -g <src_vnet-rg> -n <src_subnet> --vnet-name <vnet> --query id
----
... Create the new NIC on the destination VM with the subnet ID for the source cluster subnet. Here you enter the name for the new NIC. 
+
[source]
----
az network nic create -g <dest_node-rg> -n <dest_node-vm-nic-new> --subnet <id_from_prev_command> --accelerated-networking true
----
+
NOTE: Save the privateIPAddress. This IP address, <new_netmask_ip>, is used to create an intercluster LIF in <<Create a separate IPSpace and broadcast domain for the new NIC>>.

. Attach the new NIC to the VM.
+
[source]
----
az vm nic add -g <dest_node-rg> --vm-name <dest_node-vm> --nics <dest_node-vm-nic-new>
----
. Start the VM (node).
+
[source]
----
az vm start --resource-group <dest_node-rg>  --name <dest_node-vm>
----
. In the Azure portal, go to *Networking* and confirm that the new NIC, e.g. nic-new, exists and accelerated networking is enabled. 
+
[source]
----
az network nic list --resource-group azure-59806175-60147103-azure-rg --query "[].{NIC: name, VM: virtualMachine.id}"
----

For HA-pair deployments, repeat these steps for the partner node.

== Create a separate IPSpace and broadcast domain for the new NIC
This section provides instructions for how to create a separate IPspace and broadcast domain for the new NIC. 

A separate IPSpace for intercluster LIFs provides logical separation between networking functionality for replication between clusters. 

Use the ONTAP CLI for the following steps.

.Steps
. Create an IPSpace and broadcast domain for the new NIC (nic-new), and also create an intercluster LIF with the new NIC (nic-new) on destination:

.. Create the new IPSpace (new_ipspace).
+
[source]
----
dest::> network ipspace create -ipspace <new_ipspace>
----
.. Create a broadcast domain on the new IPSpace (new_ipspace) and add the nic-new port.
+
[source]
----
dest::> network port show
----
.. For single node systems, the newly added port is _e0b_. For HA-pair deployments with managed disks, the newly added port is _e0d_. For HA-pair deployments with page blobs, the newly added port is _e0e_. Use the node name not the VM name. Find the node name by running `node show`.  
+
[source]
----
dest::> broadcast-domain create -broadcast-domain <new_bd> -mtu 1500 -ipspace <new_ipspace> -ports <dest_node-cot-vm:e0b>
----
.. Create an intercluster LIF on the new broadcast-domain (new_bd) and on the new NIC (nic-new).
+
[source]
----
dest::> net int create -vserver <new_ipspace> -lif <new_dest_node-ic-lif> -service-policy default-intercluster -address <new_added_nic_primary_addr> -home-port <e0b> -home-node <node> -netmask <new_netmask_ip> -broadcast-domain <new_bd>
----

.. Verify creation of the new intercluster LIF.
+
[source]
----
dest::>net int show
----

For HA-pair deployments, repeat steps 2 and 3 for the partner node.

== Verify cluster peering between the source and destination systems
.Steps

. Verify that the intercluster LIF of the destination cluster can talk to the intercluster LIF or the source cluster. The destination is the intercluster LIF IP address on the source. 
+
[source]
----
dest::> ping -lif <new_dest_node-ic-lif> -vserver <new_ipspace> -destination <10.161.189.6> 
----
. Verify that the intercluster LIF of the source cluster can talk to the intercluster LIF of the destination cluster. The destination is the IP address of the new NIC created on the destination. 
+
[source]
----
src::> ping -lif <src_node-ic-lif> -vserver <src_svm> -destination <10.161.189.18>
----

For HA-pair deployments, repeat the steps for the partner node.

== Create SVM peering between the source and destination system
This section provides instructions for how to create SVM peering between the source and destination system. 

.Steps

. Create cluster peering on the destination.
+
[source]
----
dest::> cluster peer create -peer-addrs <10.161.189.6> -ipspace <new_ipspace>
----

. Enter and confirm the passphrase. *(Rachel - This wasn't mentioned in the guide but Shreyans showed this step in the demo. Does this added step provide enough information as is?)*

. Create cluster peering on the source. For systems with HA pairs, use <partner_new_nic_ip_addr> for the -peer-addrs. *(Rachel - Does this last sentence make sense?)*
+
[source]
----
src::> cluster peer create -peer-addrs <10.161.189.18>
----

. Enter and confirm the passphrase.

. Check that the cluster peered.
+
[source]
----
src::> cluster peer show 
----
+
.Output
Successful peering shows *Available* in the availability field. 

. Create SVM peering on the destination. Both source and destination SVMs should be data SVMs.  
+
[source]
----
dest::> vserver peer create -vserver <dest_svm> -peer-vserver <src_svm> -peer-cluster <src_cluster> -applications snapmirror``
----
. Accept SVM peering.
+
[source]
----
src::> vserver peer accept -vserver <src_svm> -peer-vserver <dest_svm>
----
. Check that the SVM peered.
+
[source]
----
dest::> vserver peer show
----

.Output
Peer state shows *`peered`* and peering applications shows *`snapmirror`*. 

== Create a SnapMirror replication relationship between the source and destination system
This section provides instructions for how to create a SnapMirror replication relationship between the source and destination system. 

.Steps
. Create a data protected volume on the destination SVM. 
+
[source]
----
dest::> vol create -volume <new_dest_vol> -vserver <dest_svm> -type DP -size <10GB> -aggregate <aggr1>
----
. Optional: Add an export policy rule to the volume if you want to mount the volume. This step is required for disaster recovery. *(Rachel-Is this last sentence helpful? Does it need tweaking? If we mention this, should we also refer customers to some other information about disaster recovery?)* 
+
[source]
----
dest::> vserver export-policy rule create -clientmatch 0.0.0.0/0 -policyname default -vserver <dest_svm> -rwrule any -allow-dev true -superuser any -allow-suid true -rorule any``
----
. Create the SnapMirror replication relationship on the destination which includes the SnapMirror policy and schedule for the replication.
+
[source]
----
dest::> snapmirror create -source-path src_svm:src_vol  -destination-path  dest_vs:new_dest_vol -vserver dest_svm -policy MirrorAllSnapshots -schedule 5min
----
. Initialize the SnapMirror replication relationship on the destination. 
+
[source]
----
dest::> snapmirror initialize -destination-path  <dest_vs:new_dest_vol>
----

== Validate the SnapMirror relationship is healthy
In the ONTAP CLI, run the following commands to validate the SnapMirror relationship is healthy. 

* Run the following command to learn the health of the relationship. 
+
[source]
----
dest::>snapmirror show
----
+
.Output
The relationship status is `Snapmirrored` and the health of the relationship is `true`.

* Run the following command to view the history of actions and results for the SnapMirror relationship. 
+
[source]
----
dest::>snapmirror show-history
----

Optionally, you can mount the source and destination volumes using "vol mount", write a file to the source, and verify the volume is replicating to the destination. *(Rachel: Are there commands or specific instructions for how to do this? If you want to include these steps, where would they go?)*